{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "    \n",
    "    2.) Type? Add in ID or STUDY in output    \n",
    "    3.) Time input should be changed to hrs for consistency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural PK Walkthrough\n",
    "\n",
    "This code is a rewrite of the code available at https://github.com/jameslu01/Neural_PK\n",
    "\n",
    "We have streamlined the code, and arranged its main path in this notebook.\n",
    "\n",
    "Note that the original paper and code have two different settings:\n",
    "1. Random splits of training and test.\n",
    "2. Cross-dosing splits. \n",
    "\n",
    "This notebook covers both settings. Whereas the original code base treats the two cases completely separately, this refactoring consolidates most of the functionality between the two, with minor differences as described below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import data_parse_Ping\n",
    "from data_split_Ping import data_split, data_split_cross, augment_data\n",
    "from model_utils_Ping import train_neural_ode, predict_using_trained_model, merge_predictions\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have a GPU and CUDA and pytorch with GPU support installed, this will come back as true\n",
    "# it's ok if you don't have CUDA - you can still run all of the model code, but it will be slower\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this cell, we set and describe the various parameters and hyperparameters that are used throghout the code.\n",
    "\n",
    "Note you'll find a few more parameters of the neural network hardcoded in model.py.\n",
    "\"\"\"\n",
    "\n",
    "BASE_RANDOM_SEED = 1329  # random seed for data splitting\n",
    "TORCH_RANDOM_SEED = 1000  # they have different random seeds for splitting and for the neural network\n",
    "SPLIT_FRAC = 0.2  # fraction of data reserved for testing\n",
    "OUTER_FOLDS = [1, 2, 3, 4, 5]  # indices of train/test splits\n",
    "CROSS_SCHED_FOLD = 999 # single fold value for the cross-schedule tran/test split used for setting seed and naming files\n",
    "MODEL_REPLICATES = [1, 2, 3, 4, 5 ]  # indices of model replicates for the ensemble of neural ODEs\n",
    "\n",
    "# hyperparemeters for the model, selected by grid search\n",
    "# note: the paper was not clear WHICH hyperparameters were selected by grid search\n",
    "LR = 0.001  # this is the most important hyperparameter to tune\n",
    "L2 = 0.0001  # weight decay is a form of regularization. should be tuned\n",
    "\"\"\"\n",
    "ODE solvers can approximately ensure that the output is within a given tolerance of the true solution. \n",
    "The time spent by the forward call is proportional to the number of function evaluations, \n",
    "so tuning the tolerance gives us a trade-off between accuracy and computational cost. \n",
    "Our framework allows the user to trade off speed for precision, \n",
    "but requires the user to choose an error tolerance on both the forward and reverse passes during training. \n",
    "For sequence modeling, the default value of 1.5e-8 was used. In the classification and density estimation experiments, \n",
    "we were able to reduce the tolerance to 1e-3 and 1e-5, respectively, without degrading performance.\n",
    "In short, tol is the tolerance for accepting/rejecting an adaptive step.\n",
    "\"\"\"\n",
    "TOL = 0.001\n",
    "# number of epochs to train the model. the authors use early stopping so it's not crucial.\n",
    "# just needs to be large enough so the validation loss decreases and then eventually increases\n",
    "# allowing us to identify the model state with the local minimum\n",
    "EPOCHS = 30\n",
    "# all of these together decide the size of the neural network\n",
    "HIDDEN_DIM = 32\n",
    "LATENT_DIM = 16\n",
    "ODE_HIDDEN_DIM = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ID is returning 0       100001\n",
      "1       100001\n",
      "2       100001\n",
      "3       100001\n",
      "4       100001\n",
      "         ...  \n",
      "1481    400021\n",
      "1482    400021\n",
      "1483    400021\n",
      "1484    400021\n",
      "1485    400021\n",
      "Name: ID, Length: 1486, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example data accompanying the paper. We'll import and preprocess in this cell.\n",
    "\n",
    "Data has the following columns:\n",
    "  - STUD - Study ID. Can be 1000, 2000, 3000.\n",
    "  - PTNM - Patient number. Can be repeated between studies, but for example patient 1 in Study 1000 is not the same person as patient 1 in study 2000.\n",
    "  - DSFQ - Dosage frequency is how often the dose is administred. Only 1 or 3.\n",
    "  - AMT - Dosage amount. Can be 0 when measurements taken between doses.\n",
    "  - TIME - Time since beginning of patient's treatment.\n",
    "  - TFDS - Time since dose.\n",
    "  - DV - Concentration measurement. \n",
    "\"\"\"\n",
    "data_complete = pd.read_csv(\"ExampleData/ADCdosePK.csv\", na_values=\".\")\n",
    "select_cols = [\"PTNM\", \"STUD\", \"TIME\", \"TFDS\", \"AMT\", \"DV\"]\n",
    "# According to authors: Patient data that have been marked with non-missing values in the \"C\" columns have been removed from the analysis\n",
    "#if \"C\" in data_complete.columns.values:\n",
    "    #data_complete = data_complete[data_complete.C.isnull()]\n",
    "#data_complete = data_complete[data_complete.CYCL < 100]  # cut off all dosing cycles greater than 100\n",
    "data_complete = data_complete[select_cols]  # filter down to columns of interest\n",
    "data_complete = data_complete.rename(\n",
    "    columns={\"DV\": \"PK_timeCourse\"}\n",
    ") \n",
    "\n",
    "# DV is our variable of interest - anolyte concentration\n",
    "\n",
    "data_complete[\"PTNM\"] = data_complete[\"PTNM\"].astype(\"int\").map(\"{:05d}\".format)\n",
    "data_complete[\"ID\"] = (\n",
    "    data_complete[\"STUD\"].astype(\"int\").astype(\"str\") + data_complete[\"PTNM\"])  \n",
    "print(\"The ID is returning\",data_complete[\"ID\"])\n",
    "    #concatenate study ID and patient ID for overall, unique ID\n",
    "   \n",
    "    \n",
    "time_summary = (\n",
    "    data_complete[[\"ID\", \"TIME\"]].groupby(\"ID\").max().reset_index()\n",
    ")  # get max time since start of treatment per ID\n",
    "\n",
    "# only keep patients who have measurements past initial measurements (TIME == 0)\n",
    "selected_ptnms = time_summary[time_summary.TIME > 0].ID\n",
    "\n",
    "data_complete = data_complete[data_complete.ID.isin(selected_ptnms)]\n",
    "\n",
    "data_complete[\"AMT\"] = data_complete[\"AMT\"].fillna(0)  # replace missing values for dosage with 0s\n",
    "\n",
    "# Set up round 1 measurement features.\n",
    "# Round 1 measurements for each ID are always used as input features for the neural network to predict measurements after round 1.\n",
    "# For weekly dosage IDs, round 1 is anything before end of week 1 (TIME <= 168), for every 3 week dosage IDs, anything before end of week 3 (TIME <= 604)\n",
    "#data_complete[\"PK_round1\"] = data_complete[\"PK_timeCourse\"]\n",
    "#data_complete.loc[(data_complete.DSFQ == 1) & (data_complete.TIME >= 168), \"PK_round1\"] = 0\n",
    "#data_complete.loc[(data_complete.DSFQ == 3) & (data_complete.TIME >= 504), \"PK_round1\"] = 0\n",
    "\n",
    "# Missing PK measurement value handling\n",
    "#data_complete[\"PK_round1\"] = data_complete[\"PK_round1\"].fillna(0)  # round 1 missing values filled with 0\n",
    "data_complete[\"PK_timeCourse\"] = data_complete[\"PK_timeCourse\"].fillna(-1)  \n",
    "#data=data_complete\n",
    "#all others filled with -1, used to find missing values during training\n",
    "\n",
    "#data_complete = data_complete[\n",
    "#    ((data_complete.AMT == 0) & (data_complete.TIME == 0))\n",
    "#]  # drop all first patient rows with no dosage\n",
    "\n",
    "#data=data_complete\n",
    "# Some rows are duplicate pairs for PTNM and TIME combinations with different cycle (CYCL) values\n",
    "# Set the first dosage amount of duplicated rows to the last dosage amount and keep only last row of duplicated rows\n",
    "# This implementation may be an issue if patient number (PTNM) repeats across multiple studies (STUD)\n",
    "data_complete.loc[\n",
    "    data_complete[[\"PTNM\", \"TIME\"]].duplicated(keep=\"last\"), \"AMT\"  # all non-last duplicated rows\n",
    "] = data_complete.loc[\n",
    "    data_complete[[\"PTNM\", \"TIME\"]].duplicated(keep=\"first\"), \"AMT\"  # all non-first duplicated rows\n",
    "].values\n",
    "data = data_complete[~data_complete[[\"PTNM\", \"TIME\"]].duplicated(keep=\"first\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Grid Search Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import data_parse_Ping\n",
    "import data_split_Ping\n",
    "\n",
    "import data_parse_Ping\n",
    "eval_results_all = {}\n",
    "for fold in OUTER_FOLDS:\n",
    "    for model in MODEL_REPLICATES:\n",
    "        \n",
    "        # first we split up the data into training/validation/test\n",
    "        train, test = data_split(data, \"PTNM\", seed=BASE_RANDOM_SEED + fold, test_size=SPLIT_FRAC)\n",
    "        train, validate = data_split(train, \"PTNM\", seed=BASE_RANDOM_SEED + fold + model, test_size=SPLIT_FRAC)\n",
    "        \n",
    "            \n",
    "    test_add_to_train = pd.concat(\n",
    "        [test[(test.PTNM.astype(\"int\") >= 6)]], ignore_index=True\n",
    "    )\n",
    "    train = pd.concat([train, test_add_to_train], ignore_index=True)\n",
    "    # i am not sure it makes sense to add this to the validation data?\n",
    "    validate = pd.concat([validate, test_add_to_train], ignore_index=True)\n",
    "        \n",
    "    train = augment_data(train)\n",
    "        \n",
    "from model_Ping import Encoder, ODEFunc, Classifier\n",
    "from model_utils_Ping import train_neural_ode\n",
    "\n",
    "\n",
    "# define the range of hyperparameters to search over\n",
    "param_grid = {\n",
    "    \"lr\": [0.0001, 0.001, 0.01],\n",
    "    \"tol\": [1e-3, 1e-4, 1e-5],\n",
    "    \"epochs\": [30],\n",
    "    \"l2\": [1e-4, 1e-3, 1e-2],\n",
    "    \"hidden_dim\": [32, 64, 128],\n",
    "    \"latent_dim\": [16, 32, 64],\n",
    "    \"ode_hidden_dim\": [16, 32, 64]\n",
    "}\n",
    "\n",
    "# define the number of random search iterations\n",
    "n_iter = 15\n",
    "\n",
    "# define a function that will generate a random set of hyperparameters\n",
    "def random_params(param_grid):\n",
    "    params = {}\n",
    "    for key in param_grid:\n",
    "        params[key] = random.choice(param_grid[key])\n",
    "    return params\n",
    "\n",
    "# run the random search\n",
    "for fold in OUTER_FOLDS:\n",
    "    for i in range(n_iter):\n",
    "        # generate a random set of hyperparameters\n",
    "        params = random_params(param_grid)\n",
    "    \n",
    "        # train the model using the generated hyperparameters\n",
    "        train_neural_ode(random_seed=42, train=train, validate=test, model=1, fold=6, lr = params['lr'], tol= params['tol'], epochs=30, l2=params['l2'],hidden_dim=params['hidden_dim'], latent_dim=params['latent_dim'], ode_hidden_dim=params['latent_dim'])\n",
    "        eval_results = predict_using_trained_model(\n",
    "            test,\n",
    "            model,\n",
    "            fold,\n",
    "            TOL,\n",
    "            HIDDEN_DIM,\n",
    "            LATENT_DIM,\n",
    "            ODE_HIDDEN_DIM,\n",
    "        )\n",
    "        print(f\"Iteration {i+1}/{n_iter}: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random (Non-Cross Schedule) Split W/ 5-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here the data is split, the model is trained, and then evaluated on test data. The raw results\n",
    "from this evaluatoin are saved.\n",
    "\n",
    "This entire procedure is done multiple times due to multiple training/test splits and model averaging. \n",
    "The authors of this code are doing two things when it comes to splitting the data:\n",
    "\n",
    "(1) They are doing 5 outer train/test splits. They repeat the entire training and test procedure 5 times.\n",
    "This is good practice to establish variance of the test set performance.\n",
    "(2) Within each of the 5 splits above, the are also doing model *averaging* by training 5 Neural ODE models\n",
    "that differ in: (a) initial conditions, (b) random seeds and (c) which subset of the training data is used\n",
    "for actual model training vs validation. These 5 models are then averaged together to get the final model\n",
    "which is then applied to the test set. This is standard practice to improve model quality. It also brings up\n",
    "an interesting methodological question: did they also use 5 model replicates for the other algorithms\n",
    "that they benchmarked against? If not, then their model had an unfair advantage.\n",
    "\n",
    "Feel free to skip this cell: we've already run the code and stored the results in CSVs.\n",
    "\"\"\"\n",
    "import data_parse_Ping\n",
    "eval_results_all = {}\n",
    "for fold in OUTER_FOLDS:\n",
    "    for model in MODEL_REPLICATES:\n",
    "            \n",
    "         # first we split up the data into training/validation/test\n",
    "        train, test = data_split(data, \"PTNM\", seed=BASE_RANDOM_SEED + fold, test_size=SPLIT_FRAC)\n",
    "        train, validate = data_split(train, \"PTNM\", seed=BASE_RANDOM_SEED + fold + model, test_size=SPLIT_FRAC)\n",
    "            \n",
    "        \"\"\"\n",
    "        We don't know why this is happening. We asked the authors but they did not respond.             \n",
    "        They are adding the first week of the test data to the training and validation data.\n",
    "        Although this data is not used for evaluation (see the last few lines of merge_predictions in model_utils.py), \n",
    "        it is used for training the model. Our best guess is that this provides a bit of extra training data,\n",
    "        but it's unrealistic as we won't have the first part of \"real\" test data in practice.\n",
    "        \"\"\"\n",
    "        \n",
    "        test_add_to_train = pd.concat(\n",
    "            [test[(test.PTNM.astype(\"int\") >= 15)]], ignore_index=True\n",
    "        )\n",
    "        train = pd.concat([train, test_add_to_train], ignore_index=True)\n",
    "        # i am not sure it makes sense to add this to the validation data?\n",
    "        validate = pd.concat([validate, test_add_to_train], ignore_index=True)\n",
    "        \n",
    "        \"\"\"\n",
    "        They add extra augmented data to the training set constructed out of existing training data. \n",
    "        Here is a description from the paper:\n",
    "    \n",
    "        \"We applied augmentation to prevent overfitting.\n",
    "        We applied timewise truncation to increase the number of training examples.\n",
    "        For each training example, in addition to the original example, we also truncated\n",
    "        the examples at 1008 hr, 1512 hr, and 2016 hr and generated and added\n",
    "        a set of new examples to the training examples.\"\n",
    "        \"\"\"\n",
    "            \n",
    "        train = augment_data(train)\n",
    "        train.fillna(0)\n",
    "        # create and train the model\n",
    "        # the best checkpoint will be saved\n",
    "        train_neural_ode(\n",
    "            TORCH_RANDOM_SEED + model + fold,\n",
    "            train,\n",
    "            validate,\n",
    "            model,\n",
    "            fold,\n",
    "            LR,\n",
    "            TOL,\n",
    "            EPOCHS,\n",
    "            L2,\n",
    "            HIDDEN_DIM,\n",
    "            LATENT_DIM,\n",
    "            ODE_HIDDEN_DIM\n",
    "        )\n",
    "\n",
    "        # predict on test using the best model saved\n",
    "        # during train_neural_ode\n",
    "        eval_results = predict_using_trained_model(\n",
    "            test,\n",
    "            model,\n",
    "            fold,\n",
    "            TOL,\n",
    "            HIDDEN_DIM,\n",
    "            LATENT_DIM,\n",
    "            ODE_HIDDEN_DIM,\n",
    "        )\n",
    "\n",
    "        eval_results_all[(fold, model)] = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No need to do this because we have eval_results_all from the previous cell, \n",
    "#but just a quick demonstration of how to load the predictions from the saved models\n",
    "eval_results_all_loaded = {}\n",
    "\n",
    "for fold in OUTER_FOLDS:\n",
    "    for model in MODEL_REPLICATES:\n",
    "        eval_path = os.path.join(f\"fold_{fold}\", f\"fold_{fold}_model_{model}.csv\")\n",
    "        eval_results_all_loaded[(fold, model)] = pd.read_csv(eval_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we can compute evaluation metrics and summarize them\n",
    "\n",
    "The paper discusses two of the metrics:\n",
    "\n",
    "\"We used R2 score, and Pearson correlation in this study. \n",
    "Correlation gives an intuitive estimation of the concordance between the predictions and ground truth,\n",
    "while R2 also takes into account whether the overall scale of the predictions is correct.\"\n",
    "\n",
    "However, the code also uses an additional metrics of root mean squared error (RMSE), which is a \n",
    "standard regression metric. It provides an additional view of the actual distance between the predictions\n",
    "which is not available in R2 or correlation.\n",
    "\n",
    "We have also added a new metric of the absolute difference between the predictions and the ground truth\n",
    "because we believe it is a useful viewpoint that does not have the \"squared\" distortion of the RMSE.\n",
    "\"\"\"\n",
    "r2_scores = []\n",
    "rmses = []\n",
    "pearsonrs = []\n",
    "abs_errors = []\n",
    "for fold in OUTER_FOLDS:\n",
    "    # perform the ensembling\n",
    "    evals_per_fold = [eval_results_all_loaded[(fold, m)] for m in MODEL_REPLICATES]\n",
    "    predictions = merge_predictions(evals_per_fold, data)\n",
    "    # evaluate various metrics\n",
    "    y_true = predictions[\"labels\"].values\n",
    "    y_pred = predictions[\"pred_agg\"].values\n",
    "    rmses.append(mean_squared_error(y_true, y_pred, squared=False))\n",
    "    r2_scores.append(r2_score(y_true, y_pred))\n",
    "    pearsonrs.append(pearsonr(y_true, y_pred)[0])\n",
    "    abs_errors.append(mean_absolute_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"R2\": r2_scores, \"Pearson R\": pearsonrs, \"RMSE\": rmses, \"Mean Absolute Error\": abs_errors})\n",
    "df.index = OUTER_FOLDS\n",
    "print(df, '\\n')\n",
    "\n",
    "summary_df = df.agg([\"min\", \"max\", \"mean\", \"median\"])\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Schedule Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AnneN\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\pandas\\core\\indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "--fold 999 --model 1 --lr 0.001 --tol 0.001 --epochs 30 --l2 0.0001 --hidden_dim 32 --laten_dim 16\n",
      "100%|##########| 176/176 [00:35<00:00,  4.90it/s]\n",
      "\n",
      "            Epoch 0001 | Training loss 211.755112 | Training R2 0.304944 | Validation loss 1341.792114 | Validation R2 0.300108\n",
      "            Best loss 1341.792114 | Best epoch 0001\n",
      "            \n",
      "100%|##########| 176/176 [00:42<00:00,  4.17it/s]\n",
      "\n",
      "            Epoch 0002 | Training loss 995.364624 | Training R2 -14.357357 | Validation loss 1325.067993 | Validation R2 0.317446\n",
      "            Best loss 1325.067993 | Best epoch 0002\n",
      "            \n",
      "100%|##########| 176/176 [00:44<00:00,  3.95it/s]\n",
      "\n",
      "            Epoch 0003 | Training loss 178.680527 | Training R2 0.505112 | Validation loss 1402.307617 | Validation R2 0.235553\n",
      "            Best loss 1325.067993 | Best epoch 0002\n",
      "            \n",
      "100%|##########| 176/176 [00:42<00:00,  4.11it/s]\n",
      "\n",
      "            Epoch 0004 | Training loss 168.304382 | Training R2 0.560921 | Validation loss 1691.097534 | Validation R2 -0.111727\n",
      "            Best loss 1325.067993 | Best epoch 0002\n",
      "            \n",
      "100%|##########| 176/176 [00:43<00:00,  4.07it/s]\n",
      "\n",
      "            Epoch 0005 | Training loss 160.285324 | Training R2 0.601765 | Validation loss 1628.471191 | Validation R2 -0.030910\n",
      "            Best loss 1325.067993 | Best epoch 0002\n",
      "            \n",
      "100%|##########| 176/176 [00:42<00:00,  4.12it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9320\\2667657323.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mHIDDEN_DIM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mLATENT_DIM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mODE_HIDDEN_DIM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     )\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Jupyter Neural ODE\\Neural_PK-main 1\\model_utils_Ping.py\u001b[0m in \u001b[0;36mtrain_neural_ode\u001b[1;34m(random_seed, train, validate, model, fold, lr, tol, epochs, l2, hidden_dim, latent_dim, ode_hidden_dim)\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[0madc_obj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train_dataloader\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[0madc_obj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"n_train_batches\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m                 \u001b[0mphase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             )\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Jupyter Neural ODE\\Neural_PK-main 1\\model_utils_Ping.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(encoder, ode_func, classifier, tol, latent_dim, dataloader, n_batches, phase)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;31m# load batch of data and make predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[0mptnm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmax_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mode_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mptnm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmax_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[1;31m# do not keep predictions for samples with missing labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Jupyter Neural ODE\\Neural_PK-main 1\\model_utils_Ping.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(encoder, ode_func, classifier, tol, latent_dim, ptnm, times, features, cmax_time)\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[0mtime_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtime0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# compute time interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[1;31m# use ODE solver to integrate dosing information and time interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[0msol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0modeint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mode_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_interval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m             \u001b[1;31m# feed output of ODE solver to next time point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mz0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torchdiffeq\\_impl\\adjoint.py\u001b[0m in \u001b[0;36modeint_adjoint\u001b[1;34m(func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, adjoint_params)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     ans = OdeintAdjointMethod.apply(shapes, func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol,\n\u001b[1;32m--> 199\u001b[1;33m                                     adjoint_method, adjoint_options, t.requires_grad, *adjoint_params)\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mevent_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torchdiffeq\\_impl\\adjoint.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, shapes, func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, t_requires_grad, *adjoint_params)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0modeint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0matol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevent_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mevent_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torchdiffeq\\_impl\\odeint.py\u001b[0m in \u001b[0;36modeint\u001b[1;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mevent_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0msolution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintegrate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mevent_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintegrate_until_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torchdiffeq\\_impl\\solvers.py\u001b[0m in \u001b[0;36mintegrate\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_integrate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0msolution\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_advance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msolution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torchdiffeq\\_impl\\rk_common.py\u001b[0m in \u001b[0;36m_advance\u001b[1;34m(self, next_t)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mnext_t\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_num_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'max_num_steps exceeded ({}>={})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_num_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrk_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adaptive_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mn_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_interp_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterp_coeff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrk_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torchdiffeq\\_impl\\rk_common.py\u001b[0m in \u001b[0;36m_adaptive_step\u001b[1;34m(self, rk_state)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;31m# trigger both. (i.e. interleaving them would be wrong.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_runge_kutta_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableau\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtableau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m         \u001b[1;31m# dtypes:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;31m# y1.dtype == self.y0.dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torchdiffeq\\_impl\\rk_common.py\u001b[0m in \u001b[0;36m_runge_kutta_step\u001b[1;34m(func, y0, f0, t0, dt, t1, tableau)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mperturb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerturb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0myi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta_i\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mti\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mperturb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_UncheckedAssign\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torchdiffeq\\_impl\\misc.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, t, y, perturb)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;31m# Do nothing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torchdiffeq\\_impl\\misc.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, t, y, perturb)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;31m# Do nothing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Jupyter Neural ODE\\Neural_PK-main 1\\model_Ping.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, t, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\neuralpk\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using the same building blocks, we can also train a model to predict cross-schedule PK response.\n",
    "\n",
    "In this case, we only run one outer fold, but still train 5 models within this fold. Each of the \n",
    "models has the same test split (train and validate on 3-week dosing schedule, test on 1-week dosing \n",
    "schedule), but different train/validation splits (split randomly).\n",
    "\n",
    "In this code, we have implemented the training and prediction of the split described above. The\n",
    "original code creates 2 additional test sets:\n",
    "(1) test_ineterp - the same test set, but with interpolated time points at 6 hour intervals. This was\n",
    "    done to be able to generate predictions at a higher resolution.\n",
    "(2) test_nodosing - the same test set with all dosing information masked (replaced with 0). This was\n",
    "    used to produce the analysis described in Figure 4B.\n",
    "\"\"\"\n",
    "eval_results_all = {}\n",
    "fold = CROSS_SCHED_FOLD # used for setting random seed and naming files\n",
    "for model in MODEL_REPLICATES:\n",
    "\n",
    "    # first we split up the data into training/validation/test\n",
    "    # in this case, we split 1-week schedule patients to test,\n",
    "    # then split the into train and validate randomly as above\n",
    "    train, test = data_split_cross(data)\n",
    "    train, validate = data_split(train, \"PTNM\", seed=BASE_RANDOM_SEED + fold + model, test_size=SPLIT_FRAC)\n",
    "\n",
    "    # note that we do not add any of the test data to the train\n",
    "    # and validate data as we did above\n",
    "\n",
    "    # the training data is augmented identically as above\n",
    "    train = augment_data(train)\n",
    "\n",
    "    # create and train the model\n",
    "    # the best checkpoint will be saved\n",
    "    train_neural_ode(\n",
    "        TORCH_RANDOM_SEED + model + fold,\n",
    "        train,\n",
    "        validate,\n",
    "        model,\n",
    "        fold,\n",
    "        LR,\n",
    "        TOL,\n",
    "        EPOCHS,\n",
    "        L2,\n",
    "        HIDDEN_DIM,\n",
    "        LATENT_DIM,\n",
    "        ODE_HIDDEN_DIM,\n",
    "    )\n",
    "\n",
    "    # predict on test using the best model saved\n",
    "    # during train_neural_ode\n",
    "    eval_results = predict_using_trained_model(\n",
    "        test,\n",
    "        model,\n",
    "        fold,\n",
    "        TOL,\n",
    "        HIDDEN_DIM,\n",
    "        LATENT_DIM,\n",
    "        ODE_HIDDEN_DIM,\n",
    "    )\n",
    "\n",
    "    eval_results_all[(fold, model)] = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to do this because we have eval_results_all from the previous cell, \n",
    "# but just a quick demonstration of how to load the predictions from the saved models\n",
    "eval_results_all_loaded = {}\n",
    "\n",
    "for model in MODEL_REPLICATES:\n",
    "    eval_path = os.path.join(f\"fold_{fold}\", f\"fold_{fold}_model_{model}.csv\")\n",
    "    eval_results_all_loaded[(fold, model)] = pd.read_csv(eval_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we can compute evaluation metrics same as before.\n",
    "\"\"\"\n",
    "# perform the ensembling\n",
    "evals = [eval_results_all[(fold, m)] for m in MODEL_REPLICATES]\n",
    "predictions = merge_predictions(evals, data)\n",
    "# evaluate various metrics\n",
    "y_true = predictions[\"labels\"].values\n",
    "y_pred = predictions[\"pred_agg\"].values\n",
    "\n",
    "rmse_val = mean_squared_error(y_true, y_pred, squared=False)\n",
    "r2_score_val = r2_score(y_true, y_pred)\n",
    "pearsonr_val = pearsonr(y_true, y_pred)[0]\n",
    "abs_error_val = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "metrics_vals = pd.Series({\"R2\": r2_score_val, \"Pearson R\": pearsonr_val, \"RMSE\": rmse_val, \"Mean Absolute Error\": abs_error_val})\n",
    "print(metrics_vals)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "neuralpk",
   "language": "python",
   "name": "neuralpk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
